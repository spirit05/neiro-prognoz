# model/simple_nn/trainer.py
"""
–û–±—É—á–µ–Ω–∏–µ –£–°–ò–õ–ï–ù–ù–û–ô –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List
import os
from .model import EnhancedNumberPredictor
from .data_processor import DataProcessor

class EnhancedTrainer:
    def __init__(self, model_path: str = "data/simple_model.pth"):
        self.model_path = model_path
        self.device = torch.device('cpu')
        print(f"üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.model = None
        self.criterion = nn.CrossEntropyLoss()
        
    def train(self, groups: List[str], epochs: int = 25, batch_size: int = 128) -> None:  # –£–≤–µ–ª–∏—á–∏–ª–∏ epochs –∏ batch_size
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        processor = DataProcessor(history_size=25)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –∏—Å—Ç–æ—Ä–∏—é
        features, targets = processor.prepare_training_data(groups)
        
        if len(features) == 0:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return
        
        if len(features) < 100:
            print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(features)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 100)")
            return
        
        print(f"üìä –†–∞–∑–º–µ—Ä features: {features.shape}")
        print(f"üìä –†–∞–∑–º–µ—Ä targets: {targets.shape}")
        
        # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.model = EnhancedNumberPredictor(input_size=features.shape[1])
        self.model.to(self.device)
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π optimizer —Å learning rate scheduling
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU —Ç–µ–Ω–∑–æ—Ä—ã
        features_tensor = torch.tensor(features, dtype=torch.float32)
        targets_tensor = torch.tensor(targets, dtype=torch.long) - 1
        
        print(f"üß† –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –£–°–ò–õ–ï–ù–ù–û–ô –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
        print(f"   ‚Ä¢ –≠–ø–æ—Ö–∏: {epochs}")
        print(f"   ‚Ä¢ Batch size: {batch_size}")
        print(f"   ‚Ä¢ –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(features)}")
        print(f"   ‚Ä¢ Learning rate: 0.001")
        print(f"   ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        self.model.train()
        best_loss = float('inf')
        patience_counter = 0
        patience = 5  # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        
        for epoch in range(epochs):
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö
            indices = torch.randperm(len(features))
            features_shuffled = features_tensor[indices]
            targets_shuffled = targets_tensor[indices]
            
            total_loss = 0
            num_batches = 0
            
            for i in range(0, len(features), batch_size):
                batch_end = min(i + batch_size, len(features))
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –±–∞—Ç—á–∏
                if batch_end - i < 2:
                    continue
                    
                batch_features = features_shuffled[i:batch_end]
                batch_targets = targets_shuffled[i:batch_end]
                
                self.optimizer.zero_grad()
                outputs = self.model(batch_features)
                
                # –í—ã—á–∏—Å–ª—è–µ–º loss —Å –≤–µ—Å–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏
                loss = 0
                for j in range(4):
                    loss += self.criterion(outputs[:, j, :], batch_targets[:, j])
                loss = loss / 4
                
                # L2 regularization
                l2_lambda = 0.001
                l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())
                loss = loss + l2_lambda * l2_norm
                
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                current_lr = self.optimizer.param_groups[0]['lr']
                
                print(f"üìà –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")
                
                # Learning rate scheduling
                self.scheduler.step(avg_loss)
                
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    self._save_model()
                    patience_counter = 0
                    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (loss: {avg_loss:.4f})")
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                        break
            else:
                print(f"‚ö†Ô∏è  –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –±–∞—Ç—á–µ–π")
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∏–π loss: {best_loss:.4f}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        self._analyze_model_performance(features_tensor, targets_tensor)
    
    def _analyze_model_performance(self, features_tensor: torch.Tensor, targets_tensor: torch.Tensor):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        with torch.no_grad():
            # –ë–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à–æ–π subset –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            test_size = min(1000, len(features_tensor))
            test_features = features_tensor[:test_size]
            test_targets = targets_tensor[:test_size] + 1  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ 1-26
            
            outputs = self.model(test_features)
            predictions = torch.argmax(outputs, dim=-1) + 1  # [batch_size, 4]
            
            # –í—ã—á–∏—Å–ª—è–µ–º accuracy
            correct = (predictions == test_targets).float()
            accuracy = correct.mean().item()
            
            print(f"üìä Accuracy –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {accuracy:.4f}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            unique_predictions = len(torch.unique(predictions))
            print(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª: {unique_predictions}/26")
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if self.model is not None:
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_size': self.model.feature_extractor[0].in_features,
                    'hidden_size': self.model.feature_extractor[0].out_features
                }
            }, self.model_path)