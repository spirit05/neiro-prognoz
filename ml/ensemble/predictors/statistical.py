# /opt/model/ml/ensemble/predictors/statistical.py
"""
StatisticalPredictor - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""

import numpy as np
from typing import List, Tuple, Dict
import logging
from pathlib import Path
import random

from ml.core.base_model import AbstractBaseModel
from ml.core.types import (
    ModelType, ModelStatus, TrainingConfig, 
    ModelMetadata, TrainingResult, PredictionResponse,
    DataBatch, FeatureSpec
)


class StatisticalPredictor(AbstractBaseModel):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
    
    def __init__(self, model_id: str = "statistical_predictor"):
        super().__init__(model_id, ModelType.CLASSIFICATION)
        
        self._pattern_analyzer = None
        self._max_history_length = 100
        self._feature_specs = self._create_feature_specs()
        
        self.logger.info("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω StatisticalPredictor (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)")

    def _create_feature_specs(self) -> List[FeatureSpec]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π —Ñ–∏—á"""
        return [
            FeatureSpec(name="history_length", dtype="int", required=True),
            FeatureSpec(name="pattern_complexity", dtype="float", required=True)
        ]

    def _get_pattern_analyzer(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if self._pattern_analyzer is None:
            try:
                from ml.features.engineers.advanced import AdvancedEngineer
                self._pattern_analyzer = AdvancedEngineer()
            except ImportError as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
                self._pattern_analyzer = None
        return self._pattern_analyzer

    def train(self, data: DataBatch, config: TrainingConfig) -> TrainingResult:
        """–û–±—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è"""
        self.logger.info("üîÑ –û–±—É—á–µ–Ω–∏–µ StatisticalPredictor")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –º–æ–≥—É—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if hasattr(data.data, 'values'):
            history_data = data.data.values.flatten()
            self._analyze_data_patterns(history_data)
        
        self._is_trained = True
        self.status = ModelStatus.TRAINED
        
        return TrainingResult(
            model_id=self.model_id,
            status=self.status,
            metrics={'analysis_complete': True}
        )

    def predict(self, data: DataBatch) -> PredictionResponse:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        if not self._is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        history = self._extract_history_from_batch(data)
        
        # –í–†–ï–ú–ï–ù–ù–û: —É–º–µ–Ω—å—à–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if len(history) < 10:  # –ë—ã–ª–æ 20
            self.logger.warning(f"‚ö†Ô∏è –ò—Å—Ç–æ—Ä–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è: {len(history)} —á–∏—Å–µ–ª")
            return PredictionResponse(
                predictions=[],
                model_id=self.model_id,
                inference_time=0.0
            )
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        limited_history = history[-self._max_history_length:]
        
        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        analyzer = self._get_pattern_analyzer()
        patterns = analyzer.analyze_time_series(limited_history) if analyzer else {}
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        candidates = self._generate_statistical_candidates(limited_history, patterns, top_k=10)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
        predictions = [group for group, score in candidates]
        probabilities = [[score] for group, score in candidates]
        
        self.logger.info(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(predictions)} —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
        
        return PredictionResponse(
            predictions=predictions,
            probabilities=probabilities,
            model_id=self.model_id,
            inference_time=0.0
        )

    def _extract_history_from_batch(self, data: DataBatch) -> List[int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∏–∑ DataBatch - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        try:
            if hasattr(data.data, 'values'):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ–¥–∏–Ω –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
                flattened = data.data.values.flatten()
                # –§–∏–ª—å—Ç—Ä—É–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ int
                filtered = [int(x) for x in flattened if not np.isnan(x)]
                return filtered
            else:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: {type(data.data)}")
                return []
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            return []

    def _generate_statistical_candidates(self, history: List[int], patterns: Dict, top_k: int) -> List[Tuple[Tuple[int, int, int, int], float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        candidates = []
        recent = history[-10:] if len(history) >= 10 else history
        
        # –ê–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        autocorr = patterns.get('autocorrelation', {})
        trending = patterns.get('linear_trend', 0)
        mean_reversion = patterns.get('mean_reversion', 0)
        
        self.logger.info(f"üîç –ü–∞—Ç—Ç–µ—Ä–Ω—ã: trending={trending:.3f}, mean_reversion={mean_reversion:.3f}, autocorr={len(autocorr)}")
        
        for i in range(top_k * 3):  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            group = None
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
            if abs(trending) > 0.1:
                base_nums = random.sample(recent, min(2, len(recent)))
                new_nums = [max(1, min(26, int(x + trending * random.uniform(1, 3)))) for x in base_nums]
                group = self._create_valid_group(new_nums + [random.randint(1, 26) for _ in range(2)])
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: mean reversion
            elif mean_reversion > 1.0:
                mean_val = np.mean(history)
                group = self._create_valid_group([
                    max(1, min(26, int(mean_val + random.uniform(-3, 3)))) for _ in range(4)
                ])
            
            # –°–ª—É—á–∞–π–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å —É—á–µ—Ç–æ–º –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            else:
                group = self._create_valid_group([random.randint(1, 26) for _ in range(4)])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≥—Ä—É–ø–ø–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë
            if group is not None:
                # –ë–∞–∑–æ–≤—ã–π score –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                score = 0.001 * (1 + len(autocorr) * 0.1)
                candidates.append((group, score))
                self.logger.debug(f"üîç –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∫–∞–Ω–¥–∏–¥–∞—Ç {i+1}: {group} (score={score:.6f})")
        
        self.logger.info(f"üîç –ò—Ç–æ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)}")
        
        # –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–µ—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ
        if not candidates:
            self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ")
            candidates = self._generate_fallback_candidates(top_k)
        
        return candidates[:top_k]

    def _create_valid_group(self, numbers: List[int]) -> Tuple[int, int, int, int]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ–π –≥—Ä—É–ø–ø—ã –∏–∑ —á–∏—Å–µ–ª - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        if len(numbers) < 4:
            all_nums = list(range(1, 27))
            additional = [n for n in all_nums if n not in numbers]
            numbers.extend(random.sample(additional, 4 - len(numbers)))
        
        # –°–æ–∑–¥–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –ø–∞—Ä—ã
        first_pair = numbers[:2]
        second_pair = numbers[2:4]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤ –ø–∞—Ä–∞—Ö
        if first_pair[0] == first_pair[1]:
            first_pair = (first_pair[0], random.choice([n for n in range(1, 27) if n != first_pair[0]]))
        if second_pair[0] == second_pair[1]:
            second_pair = (second_pair[0], random.choice([n for n in range(1, 27) if n != second_pair[0]]))
        
        return (first_pair[0], first_pair[1], second_pair[0], second_pair[1])

    def _generate_fallback_candidates(self, count: int) -> List[Tuple[Tuple[int, int, int, int], float]]:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        candidates = []
        
        for i in range(count):
            group = self._create_valid_group([])
            if group:
                candidates.append((group, 0.001))
        
        self.logger.info(f"üîÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(candidates)} —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        return candidates

    def _analyze_data_patterns(self, data: np.ndarray) -> None:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        if len(data) > 0:
            volatility = np.std(data)
            self.logger.info(f"üìà –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö: –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å={volatility:.2f}")

    def save(self, path: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        config = {
            'model_id': self.model_id,
            'model_type': self.model_type.value,
            'max_history_length': self._max_history_length,
            'metadata': self.metadata.model_dump(),
            'is_trained': self._is_trained
        }
        
        import json
        with open(path / "config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"üíæ StatisticalPredictor —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path}")

    def load(self, path: Path) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        config_path = path / "config.json"
        
        if not config_path.exists():
            raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {config_path}")
        
        import json
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self._max_history_length = config.get('max_history_length', 100)
        self._is_trained = config.get('is_trained', False)
        self.status = ModelStatus.READY if self._is_trained else ModelStatus.FAILED
        
        self.logger.info(f"üì• StatisticalPredictor –∑–∞–≥—Ä—É–∂–µ–Ω: {path}")
