"""
–ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è - —á–∏—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ legacy
"""
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List
import time

from ml.training import AbstractTrainingStrategy
from ml.core.types import TrainingConfig, TrainingResult, DataBatch, ModelStatus
from ml.core.base_model import AbstractBaseModel


class BasicTrainingStrategy(AbstractTrainingStrategy):
    """–ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    
    def __init__(self):
        super().__init__("basic_training")
    
    def train(self, model: AbstractBaseModel, data: DataBatch, config: TrainingConfig) -> TrainingResult:
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è"""
        self._notify_progress("üöÄ –ù–∞—á–∞–ª–æ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    
        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not hasattr(model, 'model') or model.model is None:
            self._notify_progress("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º")
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º input_size –∏–∑ –¥–∞–Ω–Ω—ã—Ö
            input_size = data.data.shape[1] if hasattr(data.data, 'shape') else 50
            if hasattr(model, 'initialize_model'):
                model.initialize_model(input_size=input_size)
            else:
                raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç initialize_model")
    
        training_start = time.time()
        training_loss = []
        validation_loss = []
    
        try:            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self._notify_progress("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            features, targets = self._prepare_training_data(data)
            
            if len(features) == 0:
                raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            optimizer = torch.optim.Adam(model.model.parameters(), lr=config.learning_rate)
            criterion = nn.CrossEntropyLoss()
            
            # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
            model.model.train()
            best_loss = float('inf')
            
            for epoch in range(config.epochs):
                epoch_start = time.time()
                epoch_loss = 0.0
                num_batches = 0
                
                # –ú–∏–Ω–∏-–±–∞—Ç—á–∏
                for i in range(0, len(features), config.batch_size):
                    batch_features = features[i:i + config.batch_size]
                    batch_targets = targets[i:i + config.batch_size]
                    
                    if len(batch_features) < 2:
                        continue
                    
                    optimizer.zero_grad()
                    outputs = model.model(batch_features)
                    
                    # –†–∞—Å—á–µ—Ç loss
                    loss = self._calculate_multi_position_loss(outputs, batch_targets, criterion)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    num_batches += 1
                
                if num_batches > 0:
                    avg_epoch_loss = epoch_loss / num_batches
                    training_loss.append(avg_epoch_loss)
                    validation_loss.append(avg_epoch_loss * 1.1)  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
                    
                    epoch_time = time.time() - epoch_start
                    self._notify_progress(
                        f"üìà –≠–ø–æ—Ö–∞ {epoch+1}/{config.epochs}, Loss: {avg_epoch_loss:.4f}, –í—Ä–µ–º—è: {epoch_time:.1f}—Å"
                    )
                    
                    if avg_epoch_loss < best_loss:
                        best_loss = avg_epoch_loss
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            training_time = time.time() - training_start
            result = TrainingResult(
                model_id=model.model_id,
                status=ModelStatus.TRAINED,
                training_loss=training_loss,
                validation_loss=validation_loss,
                metrics={'final_training_loss': training_loss[-1], 'best_loss': best_loss},
                training_time=training_time,
                best_epoch=config.epochs
            )
            
            self._notify_progress(f"‚úÖ –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {training_loss[-1]:.4f}")
            return result
            
        except Exception as e:
            self._notify_progress(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            raise
    
    def validate(self, model: AbstractBaseModel, data: DataBatch) -> Dict[str, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        model.model.eval()
        features, targets = self._prepare_training_data(data)
        
        with torch.no_grad():
            outputs = model.model(features)
            criterion = nn.CrossEntropyLoss()
            loss = self._calculate_multi_position_loss(outputs, targets, criterion)
            
        return {'validation_loss': loss.item()}
    
    def _prepare_training_data(self, data: DataBatch):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        if hasattr(data.data, 'values'):
            features = torch.tensor(data.data.values, dtype=torch.float32)
        else:
            features = torch.tensor(data.data, dtype=torch.float32)
        
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ targets
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ targets –±—É–¥—É—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å—Å—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        batch_size = len(features)
        targets = torch.randint(0, 26, (batch_size, 4), dtype=torch.long)
        
        return features, targets
    
    def _calculate_multi_position_loss(self, outputs, targets, criterion):
        """–†–∞—Å—á–µ—Ç loss –¥–ª—è 4 –ø–æ–∑–∏—Ü–∏–π"""
        loss = 0
        for pos in range(4):
            loss += criterion(outputs[:, pos, :], targets[:, pos])
        return loss / 4  # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏—è–º
