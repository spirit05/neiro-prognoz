# [file name]: ml/core/trainer.py
"""
–û–±—É—á–µ–Ω–∏–µ –£–°–ò–õ–ï–ù–ù–û–ô –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ - –ú–û–î–£–õ–¨–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø: —É—Å—Ç—Ä–∞–Ω–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Tuple
import os
import time
import gc
from config import paths, logging_config, constants

# üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç save_predictions
from ml.utils.data_utils import save_predictions

logger = logging_config.get_ml_system_logger()

class EnhancedTrainer:
    def __init__(self, model_path: str = None):
        self.model_path = model_path or paths.MODEL_FILE
        self.device = torch.device('cpu')
        self.model = None
        self.criterion = nn.CrossEntropyLoss()
        self.progress_callback = None
    
    def set_progress_callback(self, callback):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.progress_callback = callback
    
    def _report_progress(self, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"""
        logger.info(message)
        if self.progress_callback:
            self.progress_callback(message)
   
    def train(self, groups: List[str], epochs=None, batch_size=None, learning_rate=None, is_finetune=False) -> List[Tuple[Tuple[int, int, int, int], float]]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú–ò –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
        from config.constants import MAIN_TRAINING_EPOCHS, MAIN_BATCH_SIZE, MAIN_LEARNING_RATE, HIDDEN_SIZE, RETRAIN_EPOCHS, RETRAIN_LEARNING_RATE
        
        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è
        if is_finetune:
            if epochs is None:
                epochs = RETRAIN_EPOCHS
            if learning_rate is None:
                learning_rate = RETRAIN_LEARNING_RATE  # –í—ã—à–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            l2_lambda = 0.0001  # –ú–µ–Ω—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            description = "–î–û–û–ë–£–ß–ï–ù–ò–ï"
        else:
            if epochs is None:
                epochs = MAIN_TRAINING_EPOCHS
            if learning_rate is None:
                learning_rate = MAIN_LEARNING_RATE
            l2_lambda = 0.001  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            description = "–ü–û–õ–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï"
            
        if batch_size is None:
            batch_size = MAIN_BATCH_SIZE
            
        total_start_time = time.time() 

        self._report_progress(f"üöÄ {description}: {len(groups)} –≥—Ä—É–ø–ø, {epochs} —ç–ø–æ—Ö, LR={learning_rate}")

        # –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        stage1_start = time.time()
        self._report_progress("üìä –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        from ml.core.data_processor import DataProcessor
        processor = DataProcessor(history_size=25)
        features, targets = processor.prepare_training_data(groups)

        stage1_time = time.time() - stage1_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage1_time:.1f} —Å–µ–∫")

        if len(features) == 0:
            self._report_progress("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return []

        if len(features) < 50:  # üîß –£–º–µ–Ω—å—à–∏–ª –ø–æ—Ä–æ–≥ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            self._report_progress(f"‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {len(features)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
            # –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–º —á—Ç–æ –µ—Å—Ç—å

        self._report_progress(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(groups)} –≥—Ä—É–ø–ø, —Å–æ–∑–¥–∞–Ω–æ {len(features)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

        # –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        stage2_start = time.time()
        self._report_progress("üîß –≠—Ç–∞–ø 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏...")

        if self.model is None:
            from ml.core.model import EnhancedNumberPredictor
            self.model = EnhancedNumberPredictor(input_size=features.shape[1], hidden_size=HIDDEN_SIZE)
            self._report_progress("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        else:
            self._report_progress("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è")

        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

        stage2_time = time.time() - stage2_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage2_time:.1f} —Å–µ–∫")

        # –≠—Ç–∞–ø 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        stage3_start = time.time()
        self._report_progress("‚öôÔ∏è –≠—Ç–∞–ø 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞...")

        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        if is_finetune:
            # –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è: –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=epochs)
        else:
            # –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
            self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3)

        stage3_time = time.time() - stage3_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage3_time:.1f} —Å–µ–∫")

        features_tensor = torch.tensor(features, dtype=torch.float32)
        targets_tensor = torch.tensor(targets, dtype=torch.long) - 1

        # –≠—Ç–∞–ø 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        stage4_start = time.time()
        self._report_progress("üß† –≠—Ç–∞–ø 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

        self.model.train()
        best_loss = float('inf')
        patience_counter = 0
        patience = 3 if is_finetune else 5  # üîß –ú–µ–Ω—å—à–µ —Ç–µ—Ä–ø–µ–Ω–∏—è –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è

        for epoch in range(epochs):
            epoch_start_time = time.time()
            indices = torch.randperm(len(features))
            features_shuffled = features_tensor[indices]
            targets_shuffled = targets_tensor[indices]

            total_loss = 0
            num_batches = 0

            for i in range(0, len(features), batch_size):
                batch_start = i
                batch_end = min(i + batch_size, len(features))
                if batch_end - batch_start < 2:
                    continue

                batch_features = features_shuffled[batch_start:batch_end]
                batch_targets = targets_shuffled[batch_start:batch_end]

                self.optimizer.zero_grad()
                outputs = self.model(batch_features)

                loss = 0
                for j in range(4):
                    loss += self.criterion(outputs[:, j, :], batch_targets[:, j])
                loss = loss / 4

                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())
                loss = loss + l2_lambda * l2_norm

                loss.backward()
                
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
                self.optimizer.step()

                total_loss += loss.item()
                num_batches += 1

            epoch_time = time.time() - epoch_start_time

            if num_batches > 0:
                avg_loss = total_loss / num_batches
                
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ scheduler
                if is_finetune:
                    self.scheduler.step()
                    current_lr = self.scheduler.get_last_lr()[0]
                else:
                    self.scheduler.step(avg_loss)
                    current_lr = self.optimizer.param_groups[0]['lr']
                    
                self._report_progress(f"üìà –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}, –í—Ä–µ–º—è: {epoch_time:.1f} —Å–µ–∫")

                if avg_loss < best_loss:
                    best_loss = avg_loss
                    self._save_model()
                    patience_counter = 0
                    self._report_progress(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (loss: {avg_loss:.4f})")
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        self._report_progress(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                        break
            else:
                self._report_progress(f"‚ö†Ô∏è  –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –±–∞—Ç—á–µ–π")

        stage4_time = time.time() - stage4_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 4 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage4_time:.1f} —Å–µ–∫")
        self._report_progress(f"‚úÖ {description} –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∏–π loss: {best_loss:.4f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        try:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_size': self.model.input_size,
                    'hidden_size': self.model.hidden_size
                }
            }, self.model_path)
            self._report_progress("üíæ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª")
        except Exception as e:
            self._report_progress(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")

        # –≠—Ç–∞–ø 5: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        stage5_start = time.time()
        self._report_progress("üìä –≠—Ç–∞–ø 5: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        self._analyze_model_performance(features_tensor, targets_tensor)
        stage5_time = time.time() - stage5_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 5 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage5_time:.1f} —Å–µ–∫")

        # –≠—Ç–∞–ø 6: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        stage6_start = time.time()
        self._report_progress("üßπ –≠—Ç–∞–ø 6: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
        del features_tensor, targets_tensor, features_shuffled, targets_shuffled
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        stage6_time = time.time() - stage6_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 6 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage6_time:.1f} —Å–µ–∫")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        predictions = []
        try:
            if self.model is not None:
                self.model.eval()
                from ml.core.data_processor import DataProcessor
                processor = DataProcessor(history_size=25)
                recent_groups = groups[-25:] if len(groups) >= 25 else groups

                context_features = processor.create_prediction_features(recent_groups)

                if context_features is not None and len(context_features) > 0:
                    with torch.no_grad():
                        features_tensor = torch.tensor(context_features, dtype=torch.float32)
                        outputs = self.model(features_tensor)

                        for i in range(min(10, len(outputs))):
                            predicted_numbers = []
                            confidence = 1.0
                            for pos in range(4):
                                probs = torch.softmax(outputs[i, pos, :], dim=0)
                                predicted_num = torch.argmax(probs).item() + 1
                                predicted_numbers.append(predicted_num)
                                confidence *= probs[predicted_num - 1].item()
                            predictions.append((tuple(predicted_numbers), confidence))
        except Exception as e:
            self._report_progress(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {e}")

        self._report_progress(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(predictions)} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")

        # –°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        try:
            from config.constants import MAIN_TRAINING_EPOCHS, RETRAIN_EPOCHS
            from ml.learning.self_learning import SelfLearningSystem
        except ImportError as e:
            self._report_progress(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å SelfLearningSystem: {e}")
            SelfLearningSystem = None

        if 'SelfLearningSystem' in locals() and callable(SelfLearningSystem):
            try:
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–±—Ä–æ—Å –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏
                if not is_finetune and (epochs >= MAIN_TRAINING_EPOCHS):
                    learning_system = SelfLearningSystem()
                    learning_system.reset_learning_data()
                    self._report_progress("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–±—Ä–æ—à–µ–Ω–∞ –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
                else:
                    self._report_progress("üìä –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω (–¥–æ–æ–±—É—á–µ–Ω–∏–µ)")
            except Exception as e:
                self._report_progress(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±—Ä–æ—Å–µ –∞–Ω–∞–ª–∏–∑–∞: {e}")

        total_time = time.time() - total_start_time
        self._report_progress(f"üéâ –í–°–ï –≠–¢–ê–ü–´ –ó–ê–í–ï–†–®–ï–ù–´! –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
        try:
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –¢–µ–ø–µ—Ä—å save_predictions –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω
            if predictions:
                save_predictions(predictions)
                self._report_progress(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(predictions)} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤ predictions_state.json")
            else:
                self._report_progress("‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç")
        except Exception as e:
            self._report_progress(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {e}")

        return predictions
        
    def _analyze_model_performance(self, features_tensor: torch.Tensor, targets_tensor: torch.Tensor):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        self._report_progress("üîç –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")
        
        self.model.eval()
        with torch.no_grad():
            test_size = min(1000, len(features_tensor))
            test_features = features_tensor[:test_size]
            test_targets = targets_tensor[:test_size] + 1
            
            self._report_progress(f"üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {test_size} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
            
            outputs = self.model(test_features)
            predictions = torch.argmax(outputs, dim=-1) + 1
            
            correct = (predictions == test_targets).float()
            accuracy = correct.mean().item()
            
            self._report_progress(f"üìä Accuracy –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {accuracy:.4f}")
            
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            unique_predictions = len(torch.unique(predictions))
            self._report_progress(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª: {unique_predictions}/26")
            
            # –ê–Ω–∞–ª–∏–∑ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            probs = torch.softmax(outputs, dim=-1)
            entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1).mean().item()
            self._report_progress(f"üìä –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {entropy:.4f}")
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        self._report_progress("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫...")
        
        if self.model is not None:
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_size': self.model.feature_extractor[0].in_features,
                    'hidden_size': self.model.feature_extractor[0].out_features
                }
            }, self.model_path)
            
            self._report_progress(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")
        else:
            self._report_progress("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å: –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
