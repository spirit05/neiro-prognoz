# [file name]: ml/core/trainer.py
"""
–û–±—É—á–µ–Ω–∏–µ –£–°–ò–õ–ï–ù–ù–û–ô –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ - –ú–û–î–£–õ–¨–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import List, Tuple
import os
import time
import gc
from config import paths, logging_config, constants

logger = logging_config.get_ml_system_logger()

class EnhancedTrainer:
    def __init__(self, model_path: str = None):
        self.model_path = model_path or paths.MODEL_FILE
        self.device = torch.device('cpu')
        self.model = None
        self.criterion = nn.CrossEntropyLoss()
        self.progress_callback = None
    
    def set_progress_callback(self, callback):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        self.progress_callback = callback
    
    def _report_progress(self, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"""
        logger.info(message)
        if self.progress_callback:
            self.progress_callback(message)
    
    def train(self, groups: List[str], epochs=None, batch_size=None, learning_rate=None) -> List[Tuple[Tuple[int, int, int, int], float]]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        from config.constants import MAIN_TRAINING_EPOCHS, MAIN_BATCH_SIZE, MAIN_LEARNING_RATE, HIDDEN_SIZE
        if epochs is None:
            epochs = MAIN_TRAINING_EPOCHS
        if batch_size is None:
            batch_size = MAIN_BATCH_SIZE
        if learning_rate is None:
            learning_rate = MAIN_LEARNING_RATE
        total_start_time = time.time() 
        
        self._report_progress(f"üöÄ –°–¢–ê–†–¢ –æ–±—É—á–µ–Ω–∏—è: {len(groups)} –≥—Ä—É–ø–ø, {epochs} —ç–ø–æ—Ö, batch_size={batch_size}")
        
        # –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        stage1_start = time.time()
        self._report_progress("üìä –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        from ml.core.data_processor import DataProcessor
        processor = DataProcessor(history_size=25)
        features, targets = processor.prepare_training_data(groups)
        
        stage1_time = time.time() - stage1_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 1 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage1_time:.1f} —Å–µ–∫")
        
        if len(features) == 0:
            self._report_progress("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return []
        
        if len(features) < 100:
            self._report_progress(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(features)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 100)")
            return []
        
        self._report_progress(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(groups)} –≥—Ä—É–ø–ø, {len(groups)*4} —á–∏—Å–µ–ª")
        self._report_progress(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(features)} –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        stage2_start = time.time()
        self._report_progress("üîß –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        from ml.core.model import EnhancedNumberPredictor
        self.model = EnhancedNumberPredictor(input_size=features.shape[1], hidden_size=HIDDEN_SIZE)
        self.model.to(self.device)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è 4 –ì–ë RAM
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        stage2_time = time.time() - stage2_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 2 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage2_time:.1f} —Å–µ–∫")
        
        # –≠—Ç–∞–ø 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        stage3_start = time.time()
        self._report_progress("‚öôÔ∏è –≠—Ç–∞–ø 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞...")
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π optimizer —Å learning rate scheduling
        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3)
        
        stage3_time = time.time() - stage3_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 3 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage3_time:.1f} —Å–µ–∫")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU —Ç–µ–Ω–∑–æ—Ä—ã
        features_tensor = torch.tensor(features, dtype=torch.float32)
        targets_tensor = torch.tensor(targets, dtype=torch.long) - 1
        
        # –≠—Ç–∞–ø 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        stage4_start = time.time()
        self._report_progress("üß† –≠—Ç–∞–ø 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        self.model.train()
        best_loss = float('inf')
        patience_counter = 0
        patience = 5
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö
            indices = torch.randperm(len(features))
            features_shuffled = features_tensor[indices]
            targets_shuffled = targets_tensor[indices]
            
            total_loss = 0
            num_batches = 0
            
            for i in range(0, len(features), batch_size):
                batch_start = i
                batch_end = min(i + batch_size, len(features))
                
                if batch_end - batch_start < 2:
                    continue
                    
                batch_features = features_shuffled[batch_start:batch_end]
                batch_targets = targets_shuffled[batch_start:batch_end]
                
                self.optimizer.zero_grad()
                outputs = self.model(batch_features)
                
                loss = 0
                for j in range(4):
                    loss += self.criterion(outputs[:, j, :], batch_targets[:, j])
                loss = loss / 4
                
                # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                l2_lambda = 0.001
                l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())
                loss = loss + l2_lambda * l2_norm
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            epoch_time = time.time() - epoch_start_time
            
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                current_lr = self.optimizer.param_groups[0]['lr']
                
                self._report_progress(f"üìà –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}, –í—Ä–µ–º—è: {epoch_time:.1f} —Å–µ–∫")
                
                self.scheduler.step(avg_loss)
                
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    self._save_model()
                    patience_counter = 0
                    self._report_progress(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (loss: {avg_loss:.4f})")
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        self._report_progress(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                        break
            else:
                self._report_progress(f"‚ö†Ô∏è  –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}: –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –±–∞—Ç—á–µ–π")
        
        stage4_time = time.time() - stage4_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 4 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage4_time:.1f} —Å–µ–∫")
        self._report_progress(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∏–π loss: {best_loss:.4f}")
        
        # –≠—Ç–∞–ø 5: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        stage5_start = time.time()
        self._report_progress("üìä –≠—Ç–∞–ø 5: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        
        self._analyze_model_performance(features_tensor, targets_tensor)
        
        stage5_time = time.time() - stage5_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 5 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage5_time:.1f} —Å–µ–∫")
        
        # –≠—Ç–∞–ø 6: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        stage6_start = time.time()
        self._report_progress("üßπ –≠—Ç–∞–ø 6: –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        del features_tensor, targets_tensor, features_shuffled, targets_shuffled
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        stage6_time = time.time() - stage6_start
        self._report_progress(f"‚úÖ –≠—Ç–∞–ø 6 –∑–∞–≤–µ—Ä—à–µ–Ω: {stage6_time:.1f} —Å–µ–∫")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        self._report_progress("üîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è...")
        
        predictions = []
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
            if self.model is not None:
                self.model.eval()
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≥—Ä—É–ø–ø
                from ml.core.data_processor import DataProcessor
                processor = DataProcessor(history_size=25)
                
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥—Ä—É–ø–ø—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                recent_groups = groups[-25:] if len(groups) >= 25 else groups
                
                # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                context_features = processor.create_prediction_features(recent_groups)
                
                if context_features is not None and len(context_features) > 0:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã
                    with torch.no_grad():
                        features_tensor = torch.tensor(context_features, dtype=torch.float32)
                        outputs = self.model(features_tensor)
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥—ã –≤ –ø—Ä–æ–≥–Ω–æ–∑—ã
                        for i in range(min(10, len(outputs))):  # –¥–æ 10 –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
                            predicted_numbers = []
                            confidence = 1.0
                            
                            for pos in range(4):
                                probs = torch.softmax(outputs[i, pos, :], dim=0)
                                predicted_num = torch.argmax(probs).item() + 1
                                predicted_numbers.append(predicted_num)
                                confidence *= probs[predicted_num - 1].item()
                            
                            predictions.append((tuple(predicted_numbers), confidence))
                    
                    self._report_progress(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(predictions)} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")
                else:
                    self._report_progress("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∏—á–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞")
        except Exception as e:
            self._report_progress(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {e}")
        
        # üîÑ –£–ú–ù–´–ô –°–ë–†–û–° –ê–ù–ê–õ–ò–ó–ê: —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏ (–º–Ω–æ–≥–æ —ç–ø–æ—Ö)
        try:
            from config.constants import MAIN_TRAINING_EPOCHS, RETRAIN_EPOCHS
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–±—É—á–µ–Ω–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —ç–ø–æ—Ö
            is_full_training = (
                epochs >= MAIN_TRAINING_EPOCHS or 
                (epochs > RETRAIN_EPOCHS * 1.5)
            )
            
            if is_full_training:
                from ml.learning.self_learning import SelfLearningSystem
                learning_system = SelfLearningSystem()
                learning_system.reset_learning_data()
                self._report_progress("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–±—Ä–æ—à–µ–Ω–∞ –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
            else:
                self._report_progress("üìä –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω (–¥–æ–æ–±—É—á–µ–Ω–∏–µ)")
                
        except Exception as e:
            self._report_progress(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –æ–±—É—á–µ–Ω–∏—è: {e}")
        
        total_time = time.time() - total_start_time
        self._report_progress(f"üéâ –í–°–ï –≠–¢–ê–ü–´ –ó–ê–í–ï–†–®–ï–ù–´! –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫")
        
        return predictions
    
    def _analyze_model_performance(self, features_tensor: torch.Tensor, targets_tensor: torch.Tensor):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        self._report_progress("üîç –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")
        
        self.model.eval()
        with torch.no_grad():
            test_size = min(1000, len(features_tensor))
            test_features = features_tensor[:test_size]
            test_targets = targets_tensor[:test_size] + 1
            
            self._report_progress(f"üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {test_size} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
            
            outputs = self.model(test_features)
            predictions = torch.argmax(outputs, dim=-1) + 1
            
            correct = (predictions == test_targets).float()
            accuracy = correct.mean().item()
            
            self._report_progress(f"üìä Accuracy –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {accuracy:.4f}")
            
            unique_predictions = len(torch.unique(predictions))
            self._report_progress(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö —á–∏—Å–µ–ª: {unique_predictions}/26")
    
    def _save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        self._report_progress("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫...")
        
        if self.model is not None:
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'input_size': self.model.feature_extractor[0].in_features,
                    'hidden_size': self.model.feature_extractor[0].out_features
                }
            }, self.model_path)
            
            self._report_progress(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")
        else:
            self._report_progress("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å: –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
